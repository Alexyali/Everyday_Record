# 简历项目总结

## 多媒体项目

### 8.1-8.30 项目内容

建立在边缘服务器和移动客户端双端，支持高交互性的低延迟视频传输协议。提供两个功能，一个是从服务器将视频数据流实时发送到客户端，二是发送线程将从客户端反馈的信令传给编码和渲染等线程处理。

已完成部分：我们选择了quiche协议，因为它是最新的传输协议，并且比较轻量级，相比webrtc更加容易部署。存在问题是本来不适合视频传输。我们组已经实现从pipe管道中读取视频流并发送到客户端，客户端再解码播放。客户端可以返回信令到发送端。

项目要求：将quiche的代码整合到包括编码和渲染在内的发送端框架内，这样就可以在其他线程中调用client反馈的信令。

存在挑战：

- 在cmakelist中加入quiche编译依赖。

- 将原来的server代码改写为c++类并在主线程中调用。

- quiche发送回调函数直接从内存中读取数据并发送。

- quiche接收回调函数将收到的数据传回主线程。

基于quiche实现了server端的c++类设计，用于实时视频传输，并整合到视频发送端架构中。工作量包括：

- 在CMakelists里面加入quiche的依赖库和路径，这样就可以和发送端程序联合编译。

```cmake
SET(QUIC_LIB_DIR ${PROJECT_SOURCE_DIR}/deps/quic-hevc/examples/build/debug)
include_directories(${PROJECT_SOURCE_DIR}/deps/quic-hevc/include)
link_directories(${QUIC_LIB_DIR})
target_link_libraries(my_host_linux
        quiche
        ev
        dl)
```

- quiche server类包括init和loop两个函数，init是初始化，loop则是调用libev库，根据特定条件触发回调函数，例如每当文件描述符可读调用recv_cb函数，每次定时触发timeout_cb函数。

```c++
send_server->init(send_server_input, send_server_output, 1, 0, packet_cnt);
thread send_server_thread(&Send_Server::loop, std::ref(send_server));
sem_wait(&Send_Server::encoder_sem);
```

- 在quiche server类中加入视频发送回调函数，该函数利用libev的定时功能每1ms触发一次。每次从上一个线程输出的pipe中取出一个视频帧的数据并调用quic的send的api发送。

```c++
uint8_t * buf_ptr = v_enc_data;
while(v_enc_len > 0){
	int capacity = quiche_conn_stream_capacity(conn_io->conn, 4);
	int send_size = MIN(capacity, v_enc_len);
	int sent = quiche_conn_stream_send(conn_io->conn,4,
		buf_ptr,send_size,false);
	if(sent <= 0){
		fprintf(stderr,"fifo send error! flag = %d\n",sent);
		break;
	}
	fprintf(stderr,"sent %d bytes to client, send_size = %d\n",sent,send_size);
	v_enc_len -= sent;
	buf_ptr += sent;
	flush_egress(loop, conn_io);
}
delete [] v_enc_data;
```

- 利用信号量semaphore标识是否建立quiche连接，从而控制发送端其他线程是否运行，只有建立连接后才开始发送（后面好像不用这个了）

```c++
#include <semaphore.h>
sem_t sem;
# 在类中初始化为0
sem_init(&sem ,0 ,0);
# 建立连接后+1
sem_post(&sem);
# 主函数中等待
sem_wait(&sem);
# 最后销毁
sem_destory(&sem);
```

- 在quiche server类中定义static变量，用于将收到的client端指令返回给主线程。用static变量的原因是libev库要求回调函数都是静态函数，因此只能用static变量将收到的数据导出。

```c++
int *client_echo = &(Send_Server::client_echo);
```

存在的问题：

- 一开始发现发送端保存的视频和客户端收到的视频时间长度不同，debug后发现原因是ts封装视频包时pts设置不对

### 9.1-10.30 测量视频帧延迟

难点：时间同步，具体测量方法

测量方法：分别在sever端发送线程发送每一帧视频数据的时候打时间戳，client每次收到数据打一次时间戳，同时记录发送/接收的数据量。分析时只需要对比接收数据累积量和发送每一帧数据大小就可以估计出每一帧数据的传输时间。

时间同步：

ntpdate + 主机地址，可以直接修改，但是不能保证一段时间都是准确的

修改接收端主机的ntp配置文件`ntp.conf`，并将其校时服务器修改为发送端主机地址。在client上输入`ntpq -p`可以查看offset。（ntp原理是什么）

遇到的问题：理论延迟和目测延迟差距比较大，进行实验排查后发现差距在于摄像机采集时间

测试环境：无线网、LTE网络

利用zerotier进行内网穿透。客户端主机分别和校园网或者手机热点连接，这样就分别测试了无线WIFI和移动网络。

## 拥塞控制研究

- 调研了TCP BBR, PCC, Copa等前沿拥塞控制算法
- 学会使用pantheon平台，可以对比测试多个拥塞控制算法的性能
- 学会mahimahi网络仿真工具，可以模拟固定网络或者LTE网络，进行单流或者多流的测试
- 基于UDT协议实现了低延迟的拥塞控制算法，BMSB 2021在投
- 研究内容为移动蜂窝网络下可设置目标低延迟的拥塞控制算法。核心思想是控制网络排队队列的长度，保证平均延迟在目标延迟左右波动，并且充分利用网络带宽。算法主要估计RTT和用**线性回归**计算RTT梯度，再对比RTT和设定的上限和下限，根据RTT梯度的正负来判断网络拥塞情况，动态挑战拥塞窗口大小

## 2021/4 项目工作

去除libev和其他依赖库，仅根据libquiche构建client端程序（server端其他人负责）

要求：依赖项只包括`libquiche.so, quiche.h`，最终提供quiche client的类，提供send和recv的成员函数，分别从内存中发送数据和将接收数据保存到内存。

实现：

- init函数初始quiche config和host port等信息，向服务器发送第一次握手信息
- recv函数从conn中读取数据并响应（建立连接，TLS握手，接收视频数据）
- send函数将指定的信令发送回服务器

最终可以调用quiche client的类生成主函数，将收到的数据保存到pipe里，ffplay再从pipe中读取数据并发送。



## 简短总结

### 媒体融合网络智能协作计算与跨网协同传输 | 2020.3-如今

- 需求：基于低延迟传输协议，实现从边缘服务器到移动终端的低延迟高质量实时视频传输，并且支持低延迟交互，客户端可以实时请求服务端渲染视频特效或者自由切换视角。
- 分析：对比SRT协议、WebRTC、QUIC协议后选择轻量化的QUIC协议，并基于cloudflare的quiche开源库进行开发，它提供了C++的API，并且可以跨平台编译。
- 实现：
  - 首先实现了简单的视频传输demo，分为ffmpeg推流、quiche_server发送、quiche_client接收、ffplay播放四个独立模块。并且client可以发送信令给server。
  - 因为项目中采用自研编码器，需要server端反馈网络信息给编码器和渲染等线程。因此将quiche的API整合改写为支持视频发送的C++类，并和编码器、渲染等线程联合编译。最终可以从内存中读取数据并发送和将收到的信令反馈给其他线程。
  - 改写sever/client端代码，去除libev、uthash等依赖库，方便编译，并实现了简单的c++类，包括init, send和recv等函数。可以用类似socket的方式来调用并实现视频传输功能。
- 测试：利用ntp进行收发双端时间同步，并分别在局域网、WIFI和LTE网络下测试了视频传输延迟和传输带宽等网络指标
- 结果：在高带宽无线局域网下每帧延迟平均为18ms，视频画质无损失。

### 网络拥塞控制算法研究

- 背景：移动蜂窝网络的可用带宽和RTT变化非常迅速并且存在随机丢包。基于丢包的TCP拥塞控制算法在无丢包时导致过高的排队延迟，在有丢包时带宽利用率低。基于延迟的算法可以达到极低延迟，但是无法充分利用带宽
- 分析：大部分时延敏感协议不要求极低延迟，而是设定了目标延迟。因此可以在目标延迟的限制下，最大化网络吞吐量
- 实现：基于UDT协议实现，采用估计的RTT和RTT梯度作为拥塞指标，综合判断实时的网络状态，并动态调整拥塞窗口的步长，保证平均延迟低于目标延迟。并且保证协议内多流的公平性。
- 结果：在基于路径的仿真实验中，提出的算法相比BBR可以减少12.6%的延迟并只有2.8%的吞吐量损失。该论文在投BMSB 2021。

