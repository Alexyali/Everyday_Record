# 基于机器学习的端到端拥塞控制

`Machine Learning for End-to-End Congestion Control`  

`Data Science and Artificial Intelligence for Communications`  

## 介绍

端到端拥塞控制是TCP协议的一部分，用于保障网络的稳定性和资源分配的公平性。拥塞控制采用一系列预先规定的规则，比如当检测到丢包时将拥塞窗口cwnd减半，并且根据测量的RTT调整cwnd。在当前的高度动态和复杂的网络下，传统的拥塞控制算法不是很有效。

研究发现AI和网络流量控制结合可能成为下一代网络研究的前端，深度强化学习DRL会成为有效的模型。

## 挑战和机遇

TCP拥塞控制方法如下：首先启动时，终端节点需要迅速提升发送速率来提高网络资源的利用率。当检测到拥塞时，需要降低发送速率。当度过拥塞状态后，需要提高发送速率。因此发送速率根据网络状态保持上升/下降的状态。检测网络拥塞通常使用丢包和延迟作为指示器。

拥塞控制机制根据一些信号检测拥塞，并根据预先决定的规则调整CWND或者发送速率。根据用于指示拥塞的信号类型，拥塞控制可以分为几类。例如**基于丢包**的算法包括`Cubic, Reno, New Reno`。这些算法使用丢包作为网络拥塞的标志，并采用AIMD算法调整CWND。但是网络拥塞可能发生在缓冲填满之前，因此丢包并不准确。**基于延迟**的算法有`TCP Vegas`，采用数据包的延迟作为拥塞信号，但是当和基于丢包的流竞争时吞吐量很低。还有**基于混合**的算法，比如`TCP Compound`使用基于延迟的窗和基于丢包的窗的和作为拥塞窗口。BBR算法首先估计瓶颈链路最大带宽和最小RTT，通过保持吞吐量和带宽时延积BDP相等，来达到最优操作点。

基于规则的机制有如下限制：

- 适应新网络：拥塞控制算法为特定的网络设计。比如Reno为有线连接设计，丢包往往是拥塞的标志。但是无线网络下的丢包可能是链路错误产生。因此在无线网下使用该算法导致低带宽利用率。
- 从历史中学习：基于规则的方法采用一系列固定规则来处理每个情况。它没有利用过去的经验也没有假设任何先验信息。因此不能从历史中学习阻止了终端主动对拥塞情况采取行动。
- 性能：基于规则的算法是人为设计，无法真正了解网络特征，因此只能达到次优解。

## 拥塞控制和ML

ML是AL的子集，是机器在经过训练后可以在新的环境下解决任务的能力。可以分为监督学习，无监督学习和强化学习。

传统的拥塞控制只考虑丢包或者RTT作为拥塞的标志。它更容易受不可预测的因素的影响，导致不好的性能。但是基于ML的方法可以从过去的经验或者网络环境下学习，可能胜过基于规则的算法。

## 基于ML的拥塞控制算法

- Remy 将拥塞控制建模为一个决策问题。当观察到网络状态后，智能体调整CWND达到高吞吐量和低延迟的平衡。
- PCC采用在线学习，并不对网络模型做假设。

## 基于DRL的拥塞控制

在拥塞控制中，每一个节点基于观察的测量RTT和ACK调整各自的发送速率或者CWND。这可以视为决策问题。在不确定和随机环境下，决策问题通常建模为马尔可夫决策过程MDP。目标是为MDP找到最优的策略，因此期望的累计奖励最大。

一个DRL智能体基于观察到的网络状态（RTT, CWND, ACK间隔时间）调整其动作（发送速率）来和网络环境互动。训练一个DNN网络用于将状态映射到动作，因此奖励函数可以最优化。

QTCP是首先采用RL来设计拥塞控制的算法。Aurora采用一个全连接DNN学习状态-动作对。TCL-RL使用RL来动态配置拥塞控制参数。

## 性能增强

ML技术，尤其是监督学习和非监督学习有能力区分丢包类型并预测拥塞相关参数来提升性能。

- 丢包分类：无法区分丢包类型会导致差的吞吐量。
- 拥塞预测：在实践中，当拥塞发生时，已经明显影响到吞吐量性能。如果可以准确预测拥塞和拥塞相关参数，发送端可以主动应对拥塞。当前有两类方法估计拥塞相关参数：基于公式和基于历史。基于公式的方法合并了发送端测量的RTT和丢包率和CWND到一个方程来产生预测。基于历史的方法采用时间序列分析方法，例如TCP采用EWMA算法估计RTT。

有参考文献采用SVR方法预测TCP的吞吐量。文献提出了采用随机森林的丢包预测器。

## 基于ML拥塞控制的工作流

因为真实网络数据很难获取并标签，RL不需要网络数据，将会成为执行基于ML的拥塞控制的关键模型。它有能力基于实验和试错来找到最优决策，并帮助终端迅速响应环境变化。监督学习和非监督学习可以采用历史数据用于性能提升。比如可以结合DRL和监督学习。系统采用DRL用于在线决策，监督学习用于从收集的数据中提取特征。

基于ML的拥塞控制流程：首先形成决策问题。根据测量的信号和目标，合适定义状态、动作和奖励。在模型训练过程中，多样的训练方法可以用于帮助模型习得最佳的控制策略。

## 挑战和前景

- 收集真实数据：许多现存的工作依赖仿真的数据库，因此和实际系统可能不兼容。对于监督学习，收集有标签数据可能比较耗时。
- 公平性、鲁棒性和泛化能力：基于ML的拥塞控制可能对现存方法不公平。如果ML方法在一个和现存机制竞争的环境下训练，它可能会学会造成丢包来迫使其他TCP流缩减来占据更多的网络资源。另外基于ML的方法需要对快速变化的环境更加鲁棒。最后ML算法的泛化能力需要确保模型可以适应动态的网络但是不需要每次重新训练模型。
- 交叉层优化：在ML中，通常使用奖励函数作为优化目标。通常的奖励函数最大化吞吐量，最小化延迟，减少丢包率。因为可以设计奖励函数，因此更高层面的应用指标，例如用户的QoE也可以融入到奖励函数的设计中。例如在2020年，80%的互联网流量是视频相关。拥塞控制可以目标是优化用户QoE。
- 在真实环境下部署：许多现存基于DRL的拥塞控制机制在仿真环境而不是真实网络下测试。

